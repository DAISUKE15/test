{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEに基づくモデルを用いた日本語係り受け解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = load_kyoto_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_loop(s):\n",
    "    for i in range(len(s)):\n",
    "        if s[i].head == i:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub = [s for s in train if not has_loop(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_possible_heads(sentence, i):\n",
    "    \"\"\"find all index of possible modified\n",
    "    \n",
    "    arg:\n",
    "        sentence: a sentence\n",
    "        i: index of modefier\n",
    "\n",
    "    return:\n",
    "        idx(list): index list of possible modifiee\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = []\n",
    "    \n",
    "    if i == len(sentence)-1:\n",
    "        return idx\n",
    "\n",
    "    else:\n",
    "        idx.append(i+1)\n",
    "        head_next = sentence[i+1].head\n",
    "        while head_next != -1:\n",
    "            #print(head_next)\n",
    "            idx.append(head_next)\n",
    "            head_next = sentence[head_next].head\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_head_word_pair(sentence):\n",
    "    \"\"\"Find clause pair of head word in positive data\n",
    "    \n",
    "    arg:\n",
    "        sentence: train\n",
    "    return:\n",
    "        clause_pair(dict): {(clause pair):count, ...}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    clause_pair = {}\n",
    "    \n",
    "    # check all sentence in train\n",
    "    for s in sentence:\n",
    "        # check clause in sentence\n",
    "        for i in range(len(s)-1):\n",
    "            h = s[i].head\n",
    "            # check morph in i_th clause\n",
    "            for j in range(len(s[i].morphs)):\n",
    "                if s[i].morphs[j].pos_maj != \"特殊\":\n",
    "                    # check morph in h_th clause\n",
    "                    for k in range(len(s[h].morphs)):\n",
    "                        if s[h].morphs[k].pos_maj != \"特殊\":\n",
    "                            pair = (s[i].morphs[j].surface, s[h].morphs[k].surface)\n",
    "                            if pair not in clause_pair:\n",
    "                                clause_pair[pair] = 1\n",
    "                            else:\n",
    "                                clause_pair[pair] += 1\n",
    "    return clause_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_head_word(sentence):\n",
    "    \"\"\"Find head word pair, appearing more three times\n",
    "    arg:\n",
    "        sentence: train\n",
    "        \n",
    "    return:\n",
    "        head_word_set(set): head word pair appearing more three times\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    all_head_word = all_head_word_pair(sentence)\n",
    "    head_word_set = set()\n",
    "    \n",
    "    for k, v in all_head_word.items():\n",
    "        if v >= 3:\n",
    "            for x in k:\n",
    "                head_word_set.add(x)\n",
    "    return set(head_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_form(sentence, i):\n",
    "    \"\"\"extract phrase form from i_th clause\n",
    "    \n",
    "    arg:\n",
    "        sentence: a sentence\n",
    "        i: index of modifier\n",
    "        \n",
    "    return:\n",
    "        string: pre phrase form(surface)\n",
    "    \"\"\"\n",
    "    \n",
    "    for j in range(1, len(sentence[i].morphs)+1):\n",
    "        if sentence[i].morphs[-j].pos_maj != \"特殊\":\n",
    "            return sentence[i].morphs[-j].surface\n",
    "    return sentence[i].morphs[-1].surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence, i, pair_set):\n",
    "    \"\"\"extract features as string list\n",
    "\n",
    "    arg:\n",
    "        sentence: a sentence\n",
    "        i: index of modefier\n",
    "        pair(set): collect_head_word()\n",
    "        \n",
    "\n",
    "    return:\n",
    "        features(list):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    features = []\n",
    "    heads_list = find_possible_heads(sentence, i)\n",
    "    modifier = phrase_form(sentence, i)\n",
    "    \n",
    "    for h in heads_list:\n",
    "        for k in range(len(sentence[h].morphs)):\n",
    "            morphs = sentence[h].morphs[k]\n",
    "            if morphs.pos_maj != \"特殊\":\n",
    "                modified = morphs.surface\n",
    "                \n",
    "                if modifier and modified in pair_set:\n",
    "                    pair = (modifier, modified)\n",
    "                        \n",
    "                    if h == sentence[i].head:\n",
    "                        f = (pair, 1)\n",
    "                    else:\n",
    "                        f = (pair, -1)\n",
    "                    features.append(f)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(sentence):\n",
    "    \"\"\"create training_data\n",
    "\n",
    "    arg:\n",
    "        sentence: train\n",
    "    \n",
    "    return: \n",
    "        [(features, label)]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    data_train = []\n",
    "    pair = collect_head_word(sentence)\n",
    "    \n",
    "    # check all sentence in train\n",
    "    for s in sentence:\n",
    "        #check all clause in s\n",
    "        for i in range(len(s)):\n",
    "            features = extract_features(s, i, pair_set)\n",
    "            data_train.extend(features)\n",
    "    return data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = training_data(train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('は', '年頭'), -1)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_id(feature, feature_ids):\n",
    "    \"\"\"translate feature into index(number)\n",
    "    \n",
    "    arg:\n",
    "        feature: \n",
    "        feature_ids\n",
    "    \"\"\"\n",
    "    \n",
    "    if feature in feature_ids:\n",
    "        return feature_ids[feature]\n",
    "    else:\n",
    "        num = len(feature_ids)\n",
    "        feature_ids[feature] = num\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_data(data):\n",
    "    \"\"\"Create training data. (feature represented as number)\n",
    "    \n",
    "    arg:\n",
    "        data: data_train\n",
    "    \n",
    "    return:\n",
    "        n_data_train: training data represented as number\n",
    "        feature_ids(dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_ids = {} # feature name: index\n",
    "    n_data_train = [] # training data represented feature_index\n",
    "    \n",
    "    for features, label in data:\n",
    "        n_features = [] # list of feature_index\n",
    "        \n",
    "        for feature in features:\n",
    "            feature_id = get_feature_id(feature, feature_ids)\n",
    "            n_features.append(feature_id)\n",
    "            \n",
    "        n_data_train.append((n_features, label))\n",
    "\n",
    "    return n_data_train, feature_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = translate_data(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections as cl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_train = train[0]\n",
    "feature_ids = train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_data_train_to_json(n_data_train):\n",
    "    \"\"\"\n",
    "    Data(n_data_train) writing to json_file\n",
    "    \"\"\"\n",
    "    \n",
    "    data_train_list = []\n",
    "    \n",
    "    for i in range(len(n_data_train)):\n",
    "        n_data_train_dict = {}\n",
    "        n_data_train_dict['features'] = n_data_train[0][0]\n",
    "        n_data_train_dict['label'] = n_data_train[0][1]\n",
    "        data_train_list.append(n_data_train_dict)\n",
    "        \n",
    "    with open('BERT-dep/n_data_train.json', 'w') as f:\n",
    "        json.dump(data_train_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_train_to_json(n_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_id_to_json(feature_ids):\n",
    "    \"\"\"\n",
    "    Data(feature_ids) writing to json_file\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('BERT-dep/feature_ids.json', 'w') as f:\n",
    "        json.dump(feature_ids, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_id_to_json(feature_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bert_dep",
   "language": "python",
   "name": "bert_dep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
